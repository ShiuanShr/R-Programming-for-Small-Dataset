---
title: "HW4_New_RMD"
author: "Leo Shr"
date: "2022/1/12"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r} rm(list=ls())```


```{r}
#install.packages('zoo')
require(zoo)
library(data.table)
require(ggplot2)
library(dplyr)
require(tidyverse)
require(rmarkdown)
library(lubridate)
library(tidyverse)
library(dplyr)
#install.packages("factoextra")
library(factoextra)
#install.packages('DescTools')
require(DescTools)

```
(1) Import and Examine the Data 
(a) Import the CSV file into R using fread() and take a look at the data 
Import Data & dim
```{r}
retail  <- fread('onlineRetail.csv')

dim(retail)

```

Summary: 
```{r}
summary(retail)
```

head:
```{r}
head(retail)

```
__(b) Examine  the  data  by  printing  out  the  unique  number  of customers,  the  unique  number  of products purchased, as well as the unique number of transactions.__

__Preprocessing Data cleaning__

__Issues:__

- (1) The negative value of Quantity reflects the returned purchase,and it should not be removed because the returned products are not able to  be resale.


- (2) NA value in CustomerID: this part is trash cuz we are required to group data by clientID.Therefore, we should drop them .

```{r}
sum(is.na(retail))# [1] 133361
retail <- retail %>% drop_na()
```

```{r}
sum(is.na(retail))#[1] 0 , good!
```

- (3) UnitPrice: The product price per unit in sterling (£). 
However, we got negative sold price It can be solved by elimate the NA client ID.It seems that the most negative shopping record were include in the observation with no customerID

__CustomerID:__
```{r}
length(unique(retail$CustomerID)) # [1] 4372
```
__StockCode: __
```{r}
length(unique(retail$StockCode)) # [1] 3684

```
__InvoiceNo: __
```{r}
length(unique(retail$InvoiceNo )) # [1] 22190
```

__(2) Compute the RFM Variables__

- (c) Convert the InvoiceDate into a date obj. then create a variable called Recency by computing the number of days until the last day of purchase in the dataset (i.e., Dec. 09, 2011) since last purchase for each customer. 

Convert the InvoiceDate into a date obj. THen change the time into mdy_hm format then 
```{r}
data2<-retail
data2$InvoiceDate <- as.Date(mdy_hm(data2$InvoiceDate))
head(data2,3)
```
#### (d) Create a variable called Recency, Frequency and Monetary for each customer in the data. 


```{r}
data2.1<- data2
data2.2<-data2.1 %>%mutate(Amount = UnitPrice  * Quantity ) 
#For the Monetary counting

head(data2.2,3)

# "2011-12-09" the newest shopping record of the clients
# 最近有任何客人消費的時間(全部資料最後一筆)
max_date_per_user <- max(data2.2$InvoiceDate);max_date_per_user 
```


```{r}
temp = data2.2 %>% 
    group_by(CustomerID);head(temp,3)
```


#### Note:
- Monetary: it's USD, therefore we set it as float(bouble), 消費金額

- frequent= shopping count 

- Recency: 是距離當期營業日的該客戶最近消費距離多久
最近有客人消費時間- 該client最靠近現在的消費時間
```{r}
RFM <- data2.2 %>%
    group_by(CustomerID) %>%
    summarise(Monetary = as.double(sum(Amount)), 
              Frequency = n(),
              Recency = max_date_per_user- max(InvoiceDate))

head(RFM,3)
str(RFM)
```

- (3-e) Visualize the RFM variables with box plots.


```{r}
RFM_3 <- RFM
par(mfrow = c(1,3)) 
boxplot(RFM_3$Monetary, 
        main = "Monetary", 
        ylab = "$",
        horizontal = FALSE,
        col = "blue")
boxplot(RFM_3$Recency, 
        main = "Recency", 
        ylab = "Days",
        horizontal = FALSE,
        col = "green")

boxplot(RFM_3$Frequency , 
        main = "Frequency ", 
        ylab = "shoppingg number/ period",
        horizontal = FALSE,
        col = "red")
```

- (3-f) It seems that there are extreme values in the RFM variables.

__Remove these extreme values/outliers by keeping only the values that are within the 99th percentile.__


kindly remind that the function Winsorize is different from the filter method, it replaces the extreme value by not the extreme one, which is the suggested method.
Because in the process of the fitting model, we want the probability of outlier being selected as starting clustering by kmeans model is as small as it can.
Then, Winsorize function replacing the outlier with the not extreme value would make the probability distribution of the data more centralized(Variance decreasing) than the original one.
Hence, the probability of the outlier being selected as clustriod becomes slower.

```{r}
RFM_3.1 <- RFM_3
RFM_3.1_ = RFM_3.1 %>% mutate(Recency =as.numeric(RFM_3.1$Recency)) 

##filter method provided here
# No_outlier <- RFM_3.1_ %>% filter(Frequency <= quantile(Frequency, .99)) %>%
#     filter(Monetary <= quantile(Monetary, .99)) %>%
#     filter(Recency <= quantile(Recency, .99))

No_outlier <- RFM_3.1_ %>%
    mutate(Monetary   = Winsorize(Monetary  , minval = quantile(Monetary  , 0.01),
                                  maxval = quantile(Monetary, 0.99), probs = c(0.01, 0.99)),
           Frequency  = Winsorize(Frequency  , minval = quantile(Frequency  , 0.01),
                                  maxval = quantile(Frequency, 0.99), probs = c(0.01, 0.99)),
           Recency    = Winsorize(Recency  , minval = quantile(Frequency  , 0.01),
                                  maxval = quantile(Recency, 0.99), probs = c(0.01, 0.99)))
head(RFM_3.1_)
RFM_3.f_ans <- No_outlier
```

```{r}
nrow(RFM_3.1_)#[1] 4372
nrow(No_outlier)#[1] 4241
```
__comparion :Before vs  After dropping out the outlier__
_the upper part is before removing NA, 
lower part is after removing NA_

```{r}
par(mfrow = c(2,3)) 


boxplot(RFM_3$Monetary, 
        main = "Monetary", 
        ylab = "$",
        horizontal = FALSE,
        col = "blue")
boxplot(RFM_3$Recency, 
        main = "Recency", 
        ylab = "Days",
        horizontal = FALSE,
        col = "green")

boxplot(RFM_3$Frequency , 
        main = "Frequency ", 
        ylab = "shoppingg number/ period",
        horizontal = FALSE,
        col = "red")
boxplot(RFM_3.f_ans$Monetary, 
        main = "Monetary", 
        ylab = "$",
        horizontal = FALSE,
        col = "blue")
boxplot(RFM_3.f_ans$Recency, 
        main = "Recency", 
        ylab = "Days",
        horizontal = FALSE,
        col = "green")

boxplot(RFM_3.f_ans$Frequency , 
        main = "Frequency ", 
        ylab = "shoppingg number/ period",
        horizontal = FALSE,
        col = "red")
```

__(4) Scaling the Variables__
- (g) To prep the data for clustering, 
    we will need to scale the features/variables. 
    Create another data.table obj. called RFM_Scaled which contains the     CustomerID and the standardized RFM variables.

```{r}
RFM_4<- RFM_3.f_ans #不要動到前面題目答案

RFM_Scaled <- RFM_4 %>%
    mutate(Monetary= scale(Monetary),
           Frequency = scale(Frequency),
           Recency = scale(Recency)) 

RFM_Scaled <- as.data.table(RFM_Scaled); head(RFM_Scaled,3)

```

_Save the unscaled df into 'RFM_Scaled_save_Q6_l' for Question 6 beforehand_
```{r}
RFM_Scaled_save_Q6_l <- RFM_4 ; head(RFM_Scaled_save_Q6_l,3) #未scaled
```
#### (5) Running K-Means Clustering (h) Convert RFM_Scaled to a matrix. 

```{r}

RFM_Scaled.naked <- RFM_Scaled[,CustomerID:=NULL] 
#dropping  CustomerID like that  will effect the original df RFM_Scaled

matrix_RFM_Scaled <-  as.matrix(RFM_Scaled.naked)

head(matrix_RFM_Scaled, 3)#共4241筆
```
__(i) Set seed at 2021 and run k-means clustering (set k = 4).__
```{r}
set.seed(2021)
km.out <- kmeans(matrix_RFM_Scaled, 4);#km.out 
```


__(j) Attach the cluster numbers (i.e., km.out$cluster) onto RFM_Scaled.__

```{r}
Cluster_No <- km.out[1];  #get the matrix part only

RFM_Scaled_cluster <- cbind(RFM_Scaled, cluster =unlist(Cluster_No) )
#list to vector 

head(RFM_Scaled_cluster,3)
```

#### (6) Examining the Clusters
    - (k) Compute the average of RFM for each cluster.

```{r}
RFM_Scaled_cluster.6<- RFM_Scaled_cluster

Avg_RFm <- RFM_Scaled_cluster.6  %>%
    group_by(cluster)%>%
    summarise(Average_Monetary =mean(Monetary ),
              Average_Frequency = mean(Frequency), 
              Average_Recency = mean(Recency))
Avg_RFm
```
#### (Q6-2)Do we observe any difference between the clusters?
   
- Ans: 
Yes,the centriods of them are totally different and in different quadrants.

#### (Q6-3)Can we label them? 

-   Sure, we can labeled them.

    Let's Visualize our K means result first.

```{r}
fviz_cluster(km.out, data = RFM_Scaled_cluster.6[,c(1:3)],
             palette = c("#2E9FDF", "#4d1f00", "#E7B800","#ff5044" ), 
             geom = "point",
             ellipse.type = "convex", 
             ggtheme = theme_bw())
```

#### (Q6-4)Which of the clusters do you think are the most suitable for us to run target marketing campaigns and how?

-ANS:

cluster 4.
Because the clients belonging to this cluster have more Average_Monetary, shopping Frequency than others. And the negative value of Recency reflects that the cluster of the customer is our recent clients. 

(Because Recency is calculated by some fixed number - the last time user shopped)

Hence, the bigger the observation's Recency is, the longer ago that client visit the store. And the observation in a cluster will form its own average value. That is the average value(or clusteriod)we get.

To sum up, the culster 4 has a small Recency indicates that they are current customers. And with high average frequency, which means they are frequent buyers.

In conclusion, cluster 4 is suitable for market campaigns.


#### (Q6- l) Based on the list of top selling products, 

you could further develop your target marketing strategies. 
Print out the top 5 most selling products in terms of sales revenue 
(i.e., sum of sales amount = quantity x unit price) for each cluster.

```{r}
table_B<-data2.2; head(table_B,3)

```

取得有customerId, 有cluster no的運算值
```{r}
RFM_Scaled_with_cluster  <- cbind(RFM_Scaled_save_Q6_l,cluster =unlist(Cluster_No)); 
head(RFM_Scaled_with_cluster,3)
```

__Getting the desired df__
```{r}
#inner join Raw data with RFM_Scaled_with_cluster by CustomerID
full_raw_dataset <- merge(x = table_B, y = RFM_Scaled_with_cluster, by = "CustomerID", all = FALSE)
#All = True 表示Full outer Join


#str(RFM_Scaled_with_cluster)
head(full_raw_dataset, 3)
```
Part - _data cleaning for full_raw_dataset_
```{r}
#(1) remove the unnecessary col 
full_data_stage_valid_col <- full_raw_dataset
full_data_stage_valid_col <- full_data_stage_valid_col[,Description :=NULL][,Country:=NULL] 
head(full_data_stage_valid_col,3)
```

- _datatype change_
```{r}
full_data_stage_valid_col <- full_data_stage_valid_col %>%
    mutate(InvoiceNo   = as.factor(InvoiceNo),
           StockCode   = as.factor(StockCode),
           CustomerID    = as.factor(CustomerID ));
full_dataset <- full_data_stage_valid_col
str(full_data_stage_valid_col)

```

#### Print out the top 5 most selling products in terms of sales revenue 
#(i.e., sum of sales amount = quantity x unit price) for each cluster.
```{r}
cluster4_set_full.data =subset(full_dataset,cluster== 4)
cluster3_set_full.data =subset(full_dataset,cluster== 3)
cluster2_set_full.data =subset(full_dataset,cluster== 2)
cluster1_set_full.data =subset(full_dataset,cluster== 1)
```
#### cluster 1 part: 

The code below would print out the top 10 best sales for cluster 1.
However, we have to excludes the stockcode 'M','POST' because they may represent some kind of trading code or itemsets instead of specific item.
Therefore, if we neglect them, the rest top 5 best sales product are


```{r}

cluster1.top <- cluster1_set_full.data %>%
    group_by(StockCode)%>%
    summarise(Amount = sum(Amount),
              Quantity = sum(Quantity))%>%
    arrange(-(Amount))%>%  #set the order
    select(c(StockCode,Quantity, Amount))#Get the info we want via select


head(cluster1.top , 10) 

```
#### cluster 2 part: 

The top 5 product except the unknown trading code are:

```{r}
cluster2.top <- cluster2_set_full.data %>%
    group_by(StockCode)%>%
    summarise(Amount = sum(Amount),
              Quantity = sum(Quantity))%>%
    arrange(-(Amount))%>%
    select(c(StockCode,Quantity, Amount))


head(cluster2.top , 8) 
```

__Cluster 3:__

The top 5 product among the cluster 3 are:


```{r}

cluster3.top <- cluster3_set_full.data %>%
    group_by(StockCode)%>%
    summarise(Amount = sum(Amount),
              Quantity = sum(Quantity))%>%
    arrange(-(Amount))%>%
    select(c(StockCode,Quantity, Amount));head(cluster3.top , 10) 
```


    

__cluster 4 part: __

Except the special trading code, the top 5 product among the cluster 4 are:



```{r}
cluster4.top <- cluster4_set_full.data %>%
    group_by(StockCode)%>%
    summarise(Amount = sum(Amount),
              Quantity = sum(Quantity))%>%
    arrange(-(Amount))%>%
    select(c(StockCode,Quantity, Amount));head(cluster4.top , 10) 
```


### Extra Credit:

- (EC1)Any seasonality (variation by month) in purchase frequency of the 5 top/best sellers. Compute purchase frequency of the top 5 selling products by month and visualize it using ggplot2.



First, 
Our goal is to group by month, however, we got 2 Dec data. Therefore,
we shall make the time as year-month to distingish the 2010, 2011 Dec, otherwise, the value of Dec will be overestimated.
```{r}
temp <- as.yearmon(full_raw_dataset$InvoiceDate, '%Y-%m%-%d' )
```
Get the top 5 sale by amount among all the item first
```{r}

top_sale_all_year <- full_raw_dataset %>% 
    mutate(Month_ = temp)%>% #add a new column 'Month_'
    group_by(StockCode)%>%
    summarise( Total_Amount = sum(Amount),
              Total_Frequency = sum(Frequency),)%>% 
    arrange(-(Total_Amount))
    
    
Top_five <- head(top_sale_all_year,6)%>%
    filter(StockCode  != 'POST'); Top_five

# top_sale_all_year <- full_raw_dataset %>% 
#     mutate(Month_ = month(InvoiceDate) ) %>% #add a new column 'Month_'
#     group_by(StockCode)%>% 
#     summarise( Total_Amount = sum(Amount),
#               Total_Frequency = sum(Frequency),) %>%
#     arrange(-(Total_Amount))
# Top_five <- head(top_sale_all_year,6); Top_five

#Excludes StockCode 'POST' and send the vector into a vector to store
top_sale_itemset <- unlist(Top_five%>%select(StockCode))
top_sale_itemset
```
we filter out the data we need from the top_sale_itemset(the one with 'Month_')

because the data we have is 2010/12 -2011/12, therefore, we may get multiple Dec data if we only use the function month directly. We need to use Year-Date format.

```{r}
data_top_sale_only <- full_raw_dataset %>%
    mutate(Month_ = as.yearmon(full_raw_dataset$InvoiceDate, '%Y-%m%-%d' ) ) %>%
    filter(StockCode %in% top_sale_itemset) %>%
    select(c(StockCode,Quantity,Amount,Frequency,Month_))

head(data_top_sale_only,6)
```

#### Clarification

Before we start, let me clarify first:
To count how many items for the specific item were purchased during the specific month, only sum up the Quantity makes more sense.
Because if we replace it with sum(Frequency), it only reflects the trading hobby.
The frequency is the times customer visits the store and shop for a specific item.
The customer can buy a item at a massive scale at a single time. However, the frequency would only count 1 time because all of them are belong to a single transaction. And it may effect by the regional difference, instead of seasonal reason.

Therefore, the shopping quantity can reflect the seasonal demand in the market more directly in my opinion.

```{r}
Total_df <-data_top_sale_only %>%
    group_by(Month_, StockCode)%>%
    summarise(Total_Amount = sum(Amount),
               Total_Frequency = sum(Frequency),
              Total_quantity = sum(Quantity ))
head(Total_df,3)
```

plot them with different colur by group (StockCode)

```{r}
Total_plot <-Total_df %>%
    ggplot(aes(x = Month_ ,
               y = Total_quantity,colour = StockCode,group = StockCode )) +
    geom_line()+
    geom_point()+
    labs(x = 'Month',
         y = 'Frequent Quantity Count',
         title = 'Top 5 item Over the Year')
Total_plot
```


### EC2) Do we observe any seasonality?  Please explain verbally.

Yes.
The sales of the stock 85123A seems that roaring as the winter period.
And the item 47566 appears to have a high demand in the market during the period of April ~ July

Last but not least, the peak of the item 85099 seems have high demand in the winter time these two year.



####(EC3)Elbow method or the Silhouette using the packages like factoextra and NbClust. 

__Explain whether k = 4 is a reasonable decision using the Elbow/Silhouette method.__


Let see what is the current clustering status:
```{r}
fviz_cluster(km.out, data = RFM_Scaled_cluster.6[,c(1:3)],
             palette = c("#2E9FDF", "#4d1f00", "#E7B800","#ff5044" ), 
             geom = "point",
             ellipse.type = "convex", 
             ggtheme = theme_bw())

```
#### Note: 

__To achieve the better k value, our goal is to make the variance inside
a cluster as small as it can. And make the difference among the cluster as 
larger as it can__



#### Elbow Method Part:


We want to select the number of K where the WSS starts to go flat dramatically.
In this plot, I would say it quite ambiguous for the chosen of K =3 or K=4. It seems that k=4 is better than k=3.
```{r}
require(ggplot2)
fviz_nbclust(RFM_Scaled_cluster.6[,c(1:3)], 
             FUNcluster = kmeans, 
             method = "wss",     # the sum of the Squared Errors for all the points.
             k.max = 12          # max number of clusters to consider
             ) + labs(title="Elbow Method for K means")+ 
             geom_vline(xintercept = 4,linetype = 2)
```


For the part of Silhouette, this part we are testing the cohesion inside the cluster.

Appearently, as K= 4, and K=3, the average Silhouette witdth reach the high peak.

I would say it quite ambiguous for the chosen of K =3 or K=4. It seems that k=4 is slightly better than k =3.But it's not clear in this plot.

```{r}
# The more value we get from Silhouette is closer from 1, the more we desired for.

library (cluster)
set.seed(2021) 
fviz_nbclust(RFM_Scaled_cluster.6[,c(1:3)], 
             FUNcluster = kmeans, 
             method = "silhouette")
```
